{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TV Script Generation\n",
    "\n",
    "In this project, I will be generating my own [Seinfeld](https://en.wikipedia.org/wiki/Seinfeld) TV scripts using RNNs.  Dataset is from part of the [Seinfeld dataset](https://www.kaggle.com/thec03u5/seinfeld-chronicles#scripts.csv) of scripts from 9 seasons. The recurrent neural network I build will generate a new ,\"fake\" TV script, based on patterns it recognizes in this training data.\n",
    "\n",
    "## Get the Data\n",
    "\n",
    "* Access the data loaded in the `./data/Seinfeld_Scripts.txt` file\n",
    "* Take a look at some samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY CODE BELOW\n",
    "\"\"\"\n",
    "# load in data\n",
    "import helper\n",
    "data_dir = './data/Seinfeld_Scripts.txt'\n",
    "text = helper.load_data(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "Using `view_line_range` to view different parts of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Stats\n",
      "Roughly the number of unique words: 46367\n",
      "Number of lines: 109233\n",
      "Average number of words in each line: 5.544240293684143\n",
      "\n",
      "The lines 0 to 10:\n",
      "jerry: do you know what this is all about? do you know, why were here? to be out, this is out...and out is one of the single most enjoyable experiences of life. people...did you ever hear people talking about we should go out? this is what theyre talking about...this whole thing, were all out now, no one is home. not one person here is home, were all out! there are people trying to find us, they dont know where we are. (on an imaginary phone) did you ring?, i cant find him. where did he go? he didnt tell me where he was going. he must have gone out. you wanna go out you get ready, you pick out the clothes, right? you take the shower, you get all ready, get the cash, get your friends, the car, the spot, the reservation...then youre standing around, what do you do? you go we gotta be getting back. once youre out, you wanna get back! you wanna go to sleep, you wanna get up, you wanna go out again tomorrow, right? where ever you are in life, its my feeling, youve gotta go. \n",
      "\n",
      "jerry: (pointing at georges shirt) see, to me, that button is in the worst possible spot. the second button literally makes or breaks the shirt, look at it. its too high! its in no-mans-land. you look like you live with your mother. \n",
      "\n",
      "george: are you through? \n",
      "\n",
      "jerry: you do of course try on, when you buy? \n",
      "\n",
      "george: yes, it was purple, i liked it, i dont actually recall considering the buttons. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "view_line_range = (0, 10)\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY CODE BELOW\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "print('Dataset Stats')\n",
    "print('Roughly the number of unique words: {}'.format(len({word: None for word in text.split()})))\n",
    "\n",
    "lines = text.split('\\n')\n",
    "print('Number of lines: {}'.format(len(lines)))\n",
    "word_count_line = [len(line.split()) for line in lines]\n",
    "print('Average number of words in each line: {}'.format(np.average(word_count_line)))\n",
    "\n",
    "print()\n",
    "print('The lines {} to {}:'.format(*view_line_range))\n",
    "print('\\n'.join(text.split('\\n')[view_line_range[0]:view_line_range[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that all texts are lowercase, and each new line of dialogue is separated by a newline character `\\n`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Implement Pre-processing Functions\n",
    "Initial data pre-processing functions that:\n",
    "- Creates lookup table\n",
    "- Tokenize punctuation\n",
    "\n",
    "### Lookup Table\n",
    "To create a word embedding, we must transform the words to ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "import problem_unittests as tests\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def create_lookup_tables(text):\n",
    "    \"\"\"\n",
    "    Create lookup tables for vocabulary\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    text(str): The text of tv scripts split into words\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    vocab_to_int, int_to_vocab (tuple): tuple of dicts\n",
    "    \"\"\"\n",
    "    pad_id = text[-1]\n",
    "    word_counts = Counter(text[:-1])\n",
    "    # sort words from most to least frequent\n",
    "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "    # create two dictionaries: word to id, id to word\n",
    "    int_to_vocab = {ii+1: word for ii, word in enumerate(sorted_vocab)}\n",
    "    int_to_vocab[0] = pad_id\n",
    "    vocab_to_int = {word: ii for ii, word in int_to_vocab.items()}\n",
    "    \n",
    "    return vocab_to_int, int_to_vocab\n",
    "    \n",
    "\"\"\"\n",
    "DON'T MODIFY CODE BELOW\n",
    "\"\"\"\n",
    "tests.test_create_lookup_tables(create_lookup_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Punctuation\n",
    "We'll be splitting the script into a word array using spaces as delimiters.  However, punctuations like periods and exclamation marks can create multiple ids for the same word. For example, \"bye\" and \"bye!\" would generate two different word ids.\n",
    "\n",
    "The function `token_lookup` will return a dict that will be used to tokenize symbols like \"!\" into \"||Exclamation_Mark||\".  \n",
    "\n",
    "This dictionary will be used to tokenize the symbols and add the delimiter (space) around it.  This separates each symbols as its own word, making it easier for the neural network to predict the next word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def token_lookup():\n",
    "    \"\"\"\n",
    "    Generate a dict to turn punctuation into a token.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Tokenized dictionary where the key is the punctuation and the value is the token\n",
    "    \"\"\"\n",
    "    punctuations = ['.', ',', ';', '?', \n",
    "                    '!', '(', ')', '-', \n",
    "                    '\\n', '\"']\n",
    "    punctuation_names = ['Period', 'Comma', 'Semicolon', 'Question_Mark',\n",
    "                         'Exclamation_Mark', 'Left_Paren', 'Right_Paren', 'Dash',\n",
    "                         'Return', 'Quote_Mark']\n",
    "    punc_dict = {punctuations[i]: '||{}||'.format(punctuation_names[i]) \n",
    "                 for i in range(len(punctuations))}\n",
    "        \n",
    "    return punc_dict\n",
    "\n",
    "\"\"\"\n",
    "DO NOT MODIFY CODE BELOW\n",
    "\"\"\"\n",
    "tests.test_tokenize(token_lookup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process all the data and save it\n",
    "\n",
    "Running the code cell below will pre-process all the data and save it to file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DO NOT MODIFY CODE BELOW\n",
    "\"\"\"\n",
    "# pre-process training data\n",
    "helper.preprocess_and_save_data(data_dir, token_lookup, create_lookup_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "First checkpoint. The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DO NOT MODIFY CODE BELOW\n",
    "\"\"\"\n",
    "import helper\n",
    "import problem_unittests as tests\n",
    "\n",
    "int_text, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Neural Network\n",
    "Below are components necessary to build an RNN\n",
    "\n",
    "### Check Access to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DO NOT MODIFY CODE BELOW\n",
    "\"\"\"\n",
    "import torch\n",
    "\n",
    "# Check for a GPU\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if not train_on_gpu:\n",
    "    print('No GPU found. Please use a GPU to train your neural network.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input\n",
    "\n",
    "### Batching & Dataloader\n",
    "\n",
    "Using our preprocessed input data and Pytorch's TensorDataset & DataLoader method, we will load batched sequences that can be iterated.\n",
    "\n",
    "There are couple ways to do this, but I simply looped through the length (minus last sequence length to avoid indexing error) of the int_text, and appended features and corresponding output values.\n",
    "\n",
    "Here, each feature has length of indicated sequence length, while the output is the single following value in the text. Also, each appended feature is a list of integers of one index shift from before, so that it won't skip any sequence chunks. Using the DataLoader, it will still shuffle the sequences, giving a randomized inputs. In addition to that, the DataLoader method takes care of batching by our specified batch size, so that we don't need to compute beforehand.\n",
    "\n",
    "Example of batch_data function usage:\n",
    "\n",
    "Let's say we have data, `words` and desired sequence length:\n",
    "```\n",
    "words = [1, 2, 3, 4, 5, 6, 7]\n",
    "sequence_length = 4 \n",
    "```\n",
    "\n",
    "Our first `feature_tensor` should contain the values:\n",
    "```\n",
    "[1, 2, 3, 4]\n",
    "```\n",
    "And the corresponding `target_tensor` should just be the next \"word\"/tokenized word value:\n",
    "```\n",
    "5\n",
    "```\n",
    "This should continue with the second `feature_tensor`, `target_tensor` being:\n",
    "```\n",
    "[2, 3, 4, 5]  # features\n",
    "6             # target\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN SET: Get rid of double returns and join the strings back with one return\n",
    "tokens = '\\n'.join(text.lower().split('\\n\\n'))\n",
    "\n",
    "# Substitute all punctuations with token id\n",
    "for key, tok in token_dict.items():\n",
    "    tokens = tokens.replace(key, ' {} '.format(tok.lower()))\n",
    "\n",
    "# Finally, tokenize all words\n",
    "tokens = [vocab_to_int[voc] for voc in tokens.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def batch_data(tokenlist, seq_len, batch_size, shuffled=False):\n",
    "    \"\"\"\n",
    "    Batch the neural network data using DataLoader\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    tokenlist(list): The word ids of the TV scripts\n",
    "    seq_len(int): The sequence length of each batch\n",
    "    batch_size(int): The size of each batch; number of sequences in a batch\n",
    "    shuffled(bool): Setting to shuffle dataset (default: False)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    DataLoader with batched data\n",
    "    \"\"\"\n",
    "\n",
    "    # Define batch speficifications\n",
    "    batch_vol = seq_len * batch_size # Total number of elems in each batch\n",
    "    n_batches = len(tokenlist)//batch_vol # Batch counts\n",
    "    # Get tokens up to load size fit\n",
    "    tokens = tokenlist[:(n_batches*batch_vol)+seq_len]\n",
    "    # Create input batches and target batches\n",
    "    batch_x = np.array(([tokens[s:s+seq_len] for s in range(0, len(tokens)-seq_len, seq_len)])).squeeze()\n",
    "    batch_y = np.array([tokens[s+seq_len] for s in range(0, len(tokens)-seq_len, seq_len)])[:, None]\n",
    "\n",
    "    # Return Tensor type DataLoader\n",
    "    data = TensorDataset(torch.from_numpy(batch_x), torch.from_numpy(batch_y.squeeze()))\n",
    "    data_loader = DataLoader(data, shuffle=shuffled, batch_size=batch_size)\n",
    "    \n",
    "    return data_loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing batch_data function\n",
    "\n",
    "Below, we're generating some test text data and defining a dataloader using the function you defined, above. Then, we are getting some sample batch of inputs `sample_x` and targets `sample_y` from our dataloader.\n",
    "\n",
    "It should return something like the following (likely in a different order, if shuffled):\n",
    "\n",
    "```\n",
    "torch.Size([10, 5])\n",
    "tensor([[ 28,  29,  30,  31,  32],\n",
    "        [ 21,  22,  23,  24,  25],\n",
    "        [ 17,  18,  19,  20,  21],\n",
    "        [ 34,  35,  36,  37,  38],\n",
    "        [ 11,  12,  13,  14,  15],\n",
    "        [ 23,  24,  25,  26,  27],\n",
    "        [  6,   7,   8,   9,  10],\n",
    "        [ 38,  39,  40,  41,  42],\n",
    "        [ 25,  26,  27,  28,  29],\n",
    "        [  7,   8,   9,  10,  11]])\n",
    "\n",
    "torch.Size([10])\n",
    "tensor([ 33,  26,  22,  39,  16,  28,  11,  43,  30,  12])\n",
    "```\n",
    "\n",
    "### Sizes\n",
    "`sample_x` should be of size `(batch_size, sequence_length)` or (10, 5) in this case and `sample_y` should just have one dimension: batch_size (10). \n",
    "\n",
    "### Values\n",
    "The targets, `sample_y`, are the *next* value in the ordered test_text data. So, for an input sequence `[ 28,  29,  30,  31,  32]` that ends with the value `32`, the corresponding output should be `33`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 20])\n",
      "tensor([[    8,    35,     5,    28,    19,    25,    23,    51,    59,\n",
      "             4,    35,     5,    28,     3,    84,   122,    63,     4,\n",
      "             9,    55],\n",
      "        [   48,     3,    25,    23,    48,     2,     2,     2,    18,\n",
      "            48,    23,    83,    21,     7,  1253,   546,  8783,  7190,\n",
      "            21,   242],\n",
      "        [    2,   150,     2,     2,     2,    85,     5,   201,   239,\n",
      "           150,   209,    59,    56,   136,    65,    48,     4,    25,\n",
      "            23,    19],\n",
      "        [  678,   209,    59,     2,     2,     2,    25,   221,   127,\n",
      "             3,   122,    51,    48,    87,     3,    27,    83,    23,\n",
      "           290,     2],\n",
      "        [   46,    83,   375,    63,    23,   290,     3,   122,    51,\n",
      "            48,    11,    77,    49,   150,   272,     9,   249,   192,\n",
      "             3,    66],\n",
      "        [  205,    28,   129,    56,    49,     2,    12,    29,    99,\n",
      "          2745,   173,    13,    85,     5,  1367,     4,     3,     6,\n",
      "           389,   249],\n",
      "        [   70,     2,   129,    85,    36,    65,     4,    36,   527,\n",
      "            98,    26,   129,    36,    42,    81,     2,    36,   338,\n",
      "            40,   573],\n",
      "        [   48,     2,     5,   217,    65,    48,     5,    52,   428,\n",
      "             3,     5,   324,    48,     7,   559,     3,    58,     4,\n",
      "             5,   104]])\n",
      "\n",
      "torch.Size([8])\n",
      "tensor([  48,    2,  678,   46,  205,   70,   48,    7])\n"
     ]
    }
   ],
   "source": [
    "# test dataloader\n",
    "\n",
    "test_text = tokens[:1000]\n",
    "t_loader = batch_data(test_text, seq_len=20, batch_size=8)\n",
    "\n",
    "data_iter = iter(t_loader)\n",
    "sample_x, sample_y = data_iter.next()\n",
    "\n",
    "print(sample_x.shape)\n",
    "print(sample_x)\n",
    "print()\n",
    "print(sample_y.shape)\n",
    "print(sample_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Build the Neural Network\n",
    "I am going to build a recurrent neural network with LSTM, using Pytorch. The architecture consists of:\n",
    " - `__init__` - The initialize function. \n",
    " - `init_hidden` - The initialization function for a LSTM hidden state\n",
    " - `forward` - Forward propagation function.\n",
    " \n",
    "The initialize function should create the layers of the neural network and save them to the class. The forward propagation function will use these layers to run forward propagation and generate an output and a hidden state.\n",
    "\n",
    "The output of this model is the *last* batch of word scores after a complete sequence has been processed. That is, for each input sequence of words, we only want to output the word scores for a single, most likely, next word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the PyTorch RNN Module\n",
    "        \n",
    "        Params\n",
    "        ------\n",
    "        vocab_size(int): Number of input dimensions of the neural network\n",
    "            (size of the vocabulary)\n",
    "        output_size(int): Number of output dimensions of the neural network\n",
    "        embedding_dim(int): Size of embeddings, if any\n",
    "        hidden_dim(int): Size of the hidden layer outputs\n",
    "        n_layers(int): Number of hidden layers\n",
    "        dropout(float): Dropout to add in between LSTM layers\n",
    "        \"\"\"\n",
    "        super(RNN, self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embed_dim = embedding_dim\n",
    "        \n",
    "        # Embedding and LSTM layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers,\n",
    "                            dropout=dropout, batch_first=True)\n",
    "        \n",
    "        # Fully connected Linear output layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "    \n",
    "    \n",
    "    def forward(self, nn_input, hidden):\n",
    "        \"\"\"\n",
    "        Forward propagation of the neural network\n",
    "        \n",
    "        Params\n",
    "        ------\n",
    "        nn_input(Tensors): The input to the neural network\n",
    "        hidden(Tensors): The hidden state\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Two Tensors, the output of the neural network and the latest\n",
    "        hidden state\n",
    "        \"\"\"\n",
    "        batch_size = nn_input.size(0)\n",
    "        nn_input = nn_input.long()\n",
    "        embeds = self.embedding(nn_input)\n",
    "        # Get outputs and new hidden state from lstm\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        #lstm_out = self.dropout(lstm_out)\n",
    "        # Stack lstm outputs using contiguous and view to reshape\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        # Pass through fc layer\n",
    "        out = self.fc(lstm_out)\n",
    "        \n",
    "        # Reshape output to (batch_size, seq_length, output_size)\n",
    "        out = out.view(batch_size, -1, self.output_size)\n",
    "        # Get last batch output\n",
    "        final_out = out[:, -1] # so this needs to be the last value in each row\n",
    "\n",
    "        # return one batch of output word scores and the hidden state\n",
    "        return final_out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        '''\n",
    "        Initialize the hidden state of an LSTM/GRU\n",
    "        \n",
    "        Params\n",
    "        ------\n",
    "        batch_size(int): batch_size of the hidden state\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Hidden state of dimensions, (n_layers, batch_size, hidden_dim)\n",
    "        '''\n",
    "        # Two new tensors with sizes stated above, \n",
    "        # initialized to zero then move to GPU if available\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if train_on_gpu:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden\n",
    "\n",
    "\"\"\"\n",
    "DO NOT MODIFY CODE BELOW\n",
    "\"\"\"\n",
    "tests.test_rnn(RNN, train_on_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define forward and backpropagation\n",
    "\n",
    "Within the training loop, the function `forward_back_prop` will be called as demonstrated below, to implement forward and back propagation of the RNN model:\n",
    "```\n",
    "loss = forward_back_prop(decoder, decode_optim, criterion, inputs, targets)\n",
    "```\n",
    "\n",
    "And it should return the average loss over a batch and the hidden state returned by a call to `RNN(inputs, hidden)`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def forward_back_prop(decoder, decode_optim, criterion, inputs, \n",
    "                          targets, hidden, clip=0.5):\n",
    "    \"\"\"\n",
    "    Forward and backward propagation on the neural network\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    decoder: Pytorch Module that holds the recurrent neural network\n",
    "    decode_optim: Pytorch optimizer for the neural network\n",
    "    criterion: Pytorch loss function\n",
    "    inputs: A batch of input to the neural network\n",
    "    targets: The target output for the batch of input\n",
    "    clip(float): Size of gradient clipping (default=0.5)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    The loss and the latest hidden state Tensor\n",
    "    \"\"\"\n",
    "    clip=clip\n",
    "\n",
    "    if train_on_gpu:\n",
    "        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "    \n",
    "    # Initialize new hidden state variable for each batch\n",
    "    h = tuple([each.data for each in hidden])\n",
    "    decoder.zero_grad()\n",
    "    \n",
    "    # Get outputs from the model\n",
    "    output, h = decoder(inputs, h)\n",
    "    \n",
    "    # Calculate the loss and backprop\n",
    "    loss = criterion(output.squeeze(), targets)\n",
    "    loss.backward()\n",
    "    # clip gradient to prevent exploding gradient problem\n",
    "    nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
    "    optimizer.step()\n",
    "    # return the loss over a batch and the hidden state produced by our model\n",
    "    return loss.item(), h\n",
    "\n",
    "# Note that these tests aren't completely extensive.\n",
    "# they are here to act as general checks on the expected outputs of your functions\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_forward_back_prop(RNN, forward_back_prop, train_on_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Training\n",
    "\n",
    "With the structure of the network complete and data ready to be fed in the neural network, it's time to train it.\n",
    "\n",
    "### Train Loop\n",
    "\n",
    "Implemented in the `train_decoder` function, the function below will train the network over all the batches for the number of epochs given. The model progress will be shown every number of batches. This number is set with the `show_every_n_batches` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DO NOT MODIFY CODE BELOW\n",
    "\"\"\"\n",
    "\n",
    "def train_rnn(rnn, batch_size, optimizer, criterion, n_epochs, show_every_n_batches=500):\n",
    "    batch_losses = []\n",
    "    \n",
    "    rnn.train()\n",
    "\n",
    "    print(\"Training for %d epoch(s)...\" % n_epochs)\n",
    "    for epoch_i in range(1, n_epochs + 1):\n",
    "        \n",
    "        # initialize hidden state\n",
    "        hidden = rnn.init_hidden(batch_size)\n",
    "        \n",
    "        for batch_i, (inputs, labels) in enumerate(train_loader, 1):\n",
    "\n",
    "            # make sure you iterate over completely full batches, only\n",
    "            n_batches = len(train_loader.dataset)//batch_size\n",
    "            if(batch_i > n_batches):\n",
    "                break\n",
    "            \n",
    "            # forward, back prop\n",
    "            loss, hidden = forward_back_prop(rnn, optimizer, criterion, inputs, labels, hidden)          \n",
    "            # record loss\n",
    "            batch_losses.append(loss)\n",
    "\n",
    "            # printing loss stats\n",
    "            if batch_i % show_every_n_batches == 0:\n",
    "                print('Batch {} --  Epoch: {:>4}/{:<4} | Loss: {}\\n'.format(\n",
    "                    batch_i, epoch_i, n_epochs, np.average(batch_losses)))\n",
    "                batch_losses = []\n",
    "\n",
    "    # returns a trained rnn\n",
    "    return rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "Set and train the neural network with the following parameters:\n",
    "- Set `sequence_length` to the length of a sequence.\n",
    "- Set `batch_size` to the batch size.\n",
    "- Set `num_epochs` to the number of epochs to train for.\n",
    "- Set `learning_rate` to the learning rate for an Adam optimizer.\n",
    "- Set `vocab_size` to the number of uniqe tokens in our vocabulary.\n",
    "- Set `output_size` to the desired size of the output.\n",
    "- Set `embedding_dim` to the embedding dimension; smaller than the vocab_size.\n",
    "- Set `hidden_dim` to the hidden dimension of your RNN.\n",
    "- Set `n_layers` to the number of layers/cells in your RNN.\n",
    "- Set `show_every_n_batches` to the number of batches at which the neural network should print progress.\n",
    "\n",
    "- Other option is to tweak these parameters and/or the layers in the `RNN` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data params\n",
    "\n",
    "# Sequence Length (number of words in a sequence)\n",
    "sequence_length = 20\n",
    "# Batch Size\n",
    "batch_size = 32\n",
    "\n",
    "# data loader - do not change\n",
    "train_loader = batch_data(tokens, sequence_length, batch_size, shuffled=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "# Number of Epochs\n",
    "num_epochs = 20\n",
    "# Learning Rate\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Model parameters\n",
    "# Vocab size\n",
    "vocab_size = len(set(tokens))\n",
    "\n",
    "# Output size\n",
    "output_size = vocab_size\n",
    "# Embedding Dimension\n",
    "embedding_dim = 400\n",
    "# Hidden Dimension\n",
    "hidden_dim = 256\n",
    "# Number of RNN Layers\n",
    "n_layers = 2\n",
    "\n",
    "# Show stats for every n number of batches\n",
    "show_every_n_batches = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train\n",
    "\n",
    "- [x] **loss less than 3.5.** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 20 epoch(s)...\n",
      "Batch 500 --  Epoch:    1/20   | Loss: 6.045845891952514\n",
      "\n",
      "Batch 1000 --  Epoch:    1/20   | Loss: 5.4156662631034855\n",
      "\n",
      "Batch 500 --  Epoch:    2/20   | Loss: 4.966112355784614\n",
      "\n",
      "Batch 1000 --  Epoch:    2/20   | Loss: 4.589292972564698\n",
      "\n",
      "Batch 500 --  Epoch:    3/20   | Loss: 4.431584561520284\n",
      "\n",
      "Batch 1000 --  Epoch:    3/20   | Loss: 4.198808817386627\n",
      "\n",
      "Batch 500 --  Epoch:    4/20   | Loss: 4.070874601897627\n",
      "\n",
      "Batch 1000 --  Epoch:    4/20   | Loss: 3.909436241149902\n",
      "\n",
      "Batch 500 --  Epoch:    5/20   | Loss: 3.786748048397574\n",
      "\n",
      "Batch 1000 --  Epoch:    5/20   | Loss: 3.649227822780609\n",
      "\n",
      "Batch 500 --  Epoch:    6/20   | Loss: 3.542469101967198\n",
      "\n",
      "Batch 1000 --  Epoch:    6/20   | Loss: 3.417153212070465\n",
      "\n",
      "Batch 500 --  Epoch:    7/20   | Loss: 3.3240475592636827\n",
      "\n",
      "Batch 1000 --  Epoch:    7/20   | Loss: 3.2220018854141235\n",
      "\n",
      "Batch 500 --  Epoch:    8/20   | Loss: 3.1139356203303477\n",
      "\n",
      "Batch 1000 --  Epoch:    8/20   | Loss: 3.0304032826423644\n",
      "\n",
      "Batch 500 --  Epoch:    9/20   | Loss: 2.9257654991775457\n",
      "\n",
      "Batch 1000 --  Epoch:    9/20   | Loss: 2.8617904975414277\n",
      "\n",
      "Batch 500 --  Epoch:   10/20   | Loss: 2.754987702216252\n",
      "\n",
      "Batch 1000 --  Epoch:   10/20   | Loss: 2.6971192741394043\n",
      "\n",
      "Batch 500 --  Epoch:   11/20   | Loss: 2.6041001730626174\n",
      "\n",
      "Batch 1000 --  Epoch:   11/20   | Loss: 2.5501918675899504\n",
      "\n",
      "Batch 500 --  Epoch:   12/20   | Loss: 2.4639213355165897\n",
      "\n",
      "Batch 1000 --  Epoch:   12/20   | Loss: 2.420644647598267\n",
      "\n",
      "Batch 500 --  Epoch:   13/20   | Loss: 2.3505468380333174\n",
      "\n",
      "Batch 1000 --  Epoch:   13/20   | Loss: 2.2991476678848266\n",
      "\n",
      "Batch 500 --  Epoch:   14/20   | Loss: 2.2257111311549007\n",
      "\n",
      "Batch 1000 --  Epoch:   14/20   | Loss: 2.19035303068161\n",
      "\n",
      "Batch 500 --  Epoch:   15/20   | Loss: 2.1114862124518594\n",
      "\n",
      "Batch 1000 --  Epoch:   15/20   | Loss: 2.0905197336673735\n",
      "\n",
      "Batch 500 --  Epoch:   16/20   | Loss: 2.0081891274393193\n",
      "\n",
      "Batch 1000 --  Epoch:   16/20   | Loss: 1.9840537929534912\n",
      "\n",
      "Batch 500 --  Epoch:   17/20   | Loss: 1.9359137557520725\n",
      "\n",
      "Batch 1000 --  Epoch:   17/20   | Loss: 1.8945386242866515\n",
      "\n",
      "Batch 500 --  Epoch:   18/20   | Loss: 1.850276860491474\n",
      "\n",
      "Batch 1000 --  Epoch:   18/20   | Loss: 1.814054570198059\n",
      "\n",
      "Batch 500 --  Epoch:   19/20   | Loss: 1.767332058658104\n",
      "\n",
      "Batch 1000 --  Epoch:   19/20   | Loss: 1.744903353214264\n",
      "\n",
      "Batch 500 --  Epoch:   20/20   | Loss: 1.691473049545052\n",
      "\n",
      "Batch 1000 --  Epoch:   20/20   | Loss: 1.6705369406938553\n",
      "\n",
      "Model Trained and Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/torch/serialization.py:193: UserWarning: Couldn't retrieve source code for container of type RNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DO NOT MODIFY CODE BELOW\n",
    "\"\"\"\n",
    "\n",
    "# create model and move to gpu if available\n",
    "rnn = RNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "if train_on_gpu:\n",
    "    rnn.cuda()\n",
    "\n",
    "# defining loss and optimization functions for training\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# training the model\n",
    "trained_rnn = train_rnn(rnn, batch_size, optimizer, criterion, \n",
    "                        num_epochs, show_every_n_batches)\n",
    "\n",
    "# saving the trained model\n",
    "helper.save_model('./save/trained_rnn', trained_rnn)\n",
    "print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: How did you decide on your model hyperparameters? \n",
    "For example, did you try different sequence_lengths and find that one size made the model converge faster? What about your hidden_dim and n_layers; how did you decide on those?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** With my batch loading settings, training loss decreased significantly at every batch as well as every epoch, when the batch size was higher(initially tried 16). With higher batch size, I was able to maintain low variance in loss throughout 20 epochs. Larger size of hidden dim seemed to help with loss fluctuation as well. I settled for embed dimension close to the size of the batch_size * sequence_len. Learning rate seemed to work better at lower than or equal to 0.001. Greater than 2 lstm hidden layers did not yield satisfying results, in that it could not converge as fast. Although decrease in loss slowed down throughout 20 epochs, I was able to achieve loss of 1.67."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Checkpoint\n",
    "\n",
    "Model will be saved by name, `trained_rnn`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DO NOT MODIFY CODE BELOW\n",
    "\"\"\"\n",
    "import torch\n",
    "import helper\n",
    "import problem_unittests as tests\n",
    "\n",
    "_, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()\n",
    "trained_rnn = helper.load_model('./save/trained_rnn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate TV Script\n",
    "With the network trained and saved, now we can use it to generate a new, \"fake\" Seinfeld TV script in this section.\n",
    "\n",
    "### Generate Text\n",
    "To generate the text, the network needs to start with a single word and repeat its predictions until it reaches a set length. `generate` function below takes in a word id (`prime_id`) to start the script, and generates a new predicted script of specified length (`predict_len`). The prediction uses topk sampling to introduce randomness in choosing the most likely next word, given an output of word scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DO NOT MODIFY CODE BELOW\n",
    "\"\"\"\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def generate(rnn, prime_id, int_to_vocab, token_dict, pad_value, predict_len=100):\n",
    "    \"\"\"\n",
    "    Generate text using the neural network\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    rnn(nn.Module): Pytorch Module that holds the trained neural network\n",
    "    prime_id(int): The word id to start the first prediction\n",
    "    int_to_vocab(dict): Dictionary of word id keys to word values\n",
    "    token_dict(dict): Dictionary of punctuation token keys to punctuation values\n",
    "    pad_value(int): The value used to pad a sequence\n",
    "    predict_len(int): The length of text to generate\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A generated text\n",
    "    \"\"\"\n",
    "    rnn.eval()\n",
    "    \n",
    "    # create a sequence (batch_size=1) with the prime_id\n",
    "    current_seq = np.full((1, sequence_length), pad_value)\n",
    "    current_seq[-1][-1] = prime_id\n",
    "    predicted = [int_to_vocab[prime_id]]\n",
    "    \n",
    "    for _ in range(predict_len):\n",
    "        if train_on_gpu:\n",
    "            current_seq = torch.LongTensor(current_seq).cuda()\n",
    "        else:\n",
    "            current_seq = torch.LongTensor(current_seq)\n",
    "        \n",
    "        # initialize the hidden state\n",
    "        hidden = rnn.init_hidden(current_seq.size(0))\n",
    "        \n",
    "        # get the output of the rnn\n",
    "        output, _ = rnn(current_seq, hidden)\n",
    "        \n",
    "        # get the next word probabilities\n",
    "        p = F.softmax(output, dim=1).data\n",
    "        if(train_on_gpu):\n",
    "            p = p.cpu() # move to cpu\n",
    "         \n",
    "        # use top_k sampling to get the index of the next word\n",
    "        top_k = 5\n",
    "        p, top_i = p.topk(top_k)\n",
    "        top_i = top_i.numpy().squeeze()\n",
    "        \n",
    "        # select the likely next word index with some element of randomness\n",
    "        p = p.numpy().squeeze()\n",
    "        word_i = np.random.choice(top_i, p=p/p.sum())\n",
    "        \n",
    "        # retrieve that word from the dictionary\n",
    "        word = int_to_vocab[word_i]\n",
    "        predicted.append(word)     \n",
    "        \n",
    "        if(train_on_gpu):\n",
    "            current_seq = current_seq.cpu() # move to cpu\n",
    "        # the generated word becomes the next \"current sequence\" and the cycle can continue\n",
    "        if train_on_gpu:\n",
    "            current_seq = current_seq.cpu()\n",
    "\n",
    "        current_seq = np.roll(current_seq, -1, 1)\n",
    "        current_seq[-1][-1] = word_i\n",
    "    \n",
    "    gen_sentences = ' '.join(predicted)\n",
    "    \n",
    "    # Replace punctuation tokens\n",
    "    for key, token in token_dict.items():\n",
    "        ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
    "        gen_sentences = gen_sentences.replace(' ' + token.lower(), key)\n",
    "    gen_sentences = gen_sentences.replace('\\n ', '\\n')\n",
    "    gen_sentences = gen_sentences.replace('( ', '(')\n",
    "    \n",
    "    # return all the sentences\n",
    "    return gen_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a New Script\n",
    "Set `gen_length` to the length of TV script to be generated and using one of the `prime_word`'s below as a starter of the prediction:\n",
    "- \"jerry\"\n",
    "- \"elaine\"\n",
    "- \"george\"\n",
    "- \"kramer\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:55: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jerry:, all don't say that bad money?\n",
      "kramer: yeah.\n",
      "jerry: oh, i'm satisfied.\n",
      "kramer: well, yeah, i'm satisfied of purses calculators with that guy.\n",
      "kramer:(immaturely elaine) yeah- hey, hey- hey! hey, hey! hey.\n",
      "man:(still jerry) hey, hey, look at you look at you!\n",
      "jerry: ok.\n",
      "kramer: our atkins at puddy place, and i said you could've with that. so, look, i guess i will get a lo out from my ears, why know what i have to talk?\n",
      "george: i guess no i said.. i'll pick you, and you know, you know, but you could get out there for your milk. and he could be more men up his apartment, but i said, i'm going to be nature that.\n",
      "elaine:(still confused every waitress) rusty burning, baby, we have to look at these!\n",
      "george: slippery do you had to pay?\n",
      "elaine: he didnt a\n"
     ]
    }
   ],
   "source": [
    "gen_length = 200 # modify the length to your preference\n",
    "prime_word = 'jerry:' # name for starting the script\n",
    "\n",
    "\"\"\"\n",
    "DO NOT MODIFY CODE BELOW\n",
    "\"\"\"\n",
    "pad_word = helper.SPECIAL_WORDS['PADDING']\n",
    "generated_script = generate(trained_rnn, vocab_to_int[prime_word], int_to_vocab, token_dict, vocab_to_int[pad_word], gen_length)\n",
    "print(generated_script)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save script to a text file\n",
    "f =  open(\"generated_script_1.txt\",\"w\")\n",
    "f.write(generated_script)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
